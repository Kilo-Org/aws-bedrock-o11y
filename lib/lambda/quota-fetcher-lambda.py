import json
import boto3
import logging
import time
from datetime import datetime
from botocore.exceptions import ClientError

logger = logging.getLogger()
logger.setLevel(logging.INFO)

# Lambda function to fetch AWS Service Quotas for Bedrock models and publish as CloudWatch metrics
# 
# Expected event structure:
# - For Custom Resource events: event['ResourceProperties']['models']
# - For EventBridge events: event['models']
# 
# Each model object should contain:
# - modelId: Full model ID with endpoint prefix (e.g., 'us.amazon.nova-micro-v1:0')
# - tokenQuotaCode: AWS Service Quota code for token limits (e.g., 'L-DC7FF66C')
# - requestQuotaCode: AWS Service Quota code for request limits (e.g., 'L-3F110E0F')
#
# The data structure is generated by the CDK stack using the registry-based approach
# from lib/bedrock-registries.ts

# Initialize clients outside handler for connection reuse
sq_client = boto3.client('service-quotas')
cw_client = boto3.client('cloudwatch')

def get_specific_quotas(service_code, quota_codes, max_retries=3):
    """
    Fetch specific quotas by quota code instead of listing all quotas.
    Much more efficient when we only need a small subset of quotas.
    
    GetServiceQuota rate limit: 5 requests/second with 5 burst
    For ~30 quota codes, this takes ~6 seconds vs ~30+ seconds for ListServiceQuotas
    """
    logger.info(f'Fetching {len(quota_codes)} specific quotas for service: {service_code}')
    quotas = {}
    failed_quotas = []
    
    # Process quotas in batches to respect rate limits
    batch_size = 5  # Match the rate limit
    total_batches = (len(quota_codes) + batch_size - 1) // batch_size
    
    start_time = time.time()
    
    for batch_num in range(total_batches):
        batch_start = batch_num * batch_size
        batch_end = min(batch_start + batch_size, len(quota_codes))
        batch_codes = quota_codes[batch_start:batch_end]
        
        logger.info(f'Processing batch {batch_num + 1}/{total_batches} ({len(batch_codes)} quotas)')
        
        # Process each quota in the batch
        for quota_code in batch_codes:
            for attempt in range(max_retries):
                try:
                    response = sq_client.get_service_quota(
                        ServiceCode=service_code,
                        QuotaCode=quota_code
                    )
                    
                    quota_value = response.get('Quota', {}).get('Value')
                    if quota_value is not None:
                        quotas[quota_code] = quota_value
                        logger.debug(f'Fetched quota {quota_code}: {quota_value}')
                    else:
                        logger.warning(f'No value found for quota {quota_code}')
                        failed_quotas.append(quota_code)
                    
                    break  # Success, move to next quota
                    
                except ClientError as error:
                    error_code = error.response['Error']['Code']
                    
                    if error_code in ['TooManyRequestsException', 'ThrottlingException']:
                        if attempt < max_retries - 1:
                            wait_time = min(2, (2 ** attempt))  # Exponential backoff: 1s, 2s
                            logger.warning(f'{error_code} for {quota_code}, retrying in {wait_time}s (attempt {attempt + 1}/{max_retries})')
                            time.sleep(wait_time)
                            continue
                        else:
                            logger.error(f'Max retries exceeded for quota {quota_code} after {error_code}')
                            failed_quotas.append(quota_code)
                            break
                    elif error_code == 'NoSuchResourceException':
                        logger.warning(f'Quota {quota_code} not found in {service_code}')
                        failed_quotas.append(quota_code)
                        break
                    else:
                        logger.error(f'Unexpected error for quota {quota_code}: {error}')
                        failed_quotas.append(quota_code)
                        break
        
        # Add delay between batches to respect rate limits (5 requests/second)
        if batch_num < total_batches - 1:  # Don't delay after last batch
            time.sleep(1.1)  # Slightly over 1 second to be safe with rate limits
    
    fetch_time = time.time() - start_time
    
    success_count = len(quotas)
    failed_count = len(failed_quotas)
    
    logger.info(f'Quota fetch complete: {success_count} successful, {failed_count} failed in {fetch_time:.2f}s')
    
    if failed_quotas:
        logger.warning(f'Failed to fetch quotas: {failed_quotas}')
    
    return quotas

def publish_metrics_batch(metrics_data):
    """
    Publish metrics in batches to avoid CloudWatch API limits.
    CloudWatch PutMetricData supports up to 20 metrics per call.
    """
    batch_size = 20
    total_metrics = len(metrics_data)
    
    for i in range(0, total_metrics, batch_size):
        batch = metrics_data[i:i + batch_size]
        
        try:
            cw_client.put_metric_data(
                Namespace='Bedrock/Quotas',
                MetricData=batch
            )
            logger.info(f'Published batch of {len(batch)} metrics (batch {i//batch_size + 1})')
            
            # Small delay between batches to avoid rate limiting
            if i + batch_size < total_metrics:
                time.sleep(0.1)  # 100ms delay between batches
                
        except ClientError as error:
            logger.error(f'Error publishing metrics batch: {error}')
            raise

def handler(event, context):
    logger.info(f'Event: {json.dumps(event)}')
    
    # Determine event type
    is_custom_resource = 'RequestType' in event
    
    # Handle Delete events for Custom Resources
    if is_custom_resource and event['RequestType'] == 'Delete':
        return {
            'PhysicalResourceId': event.get('PhysicalResourceId', 'initial-quota-fetch'),
            'Data': {}
        }
    
    # Get models from the appropriate location
    models = event['ResourceProperties'].get('models', []) if is_custom_resource else event.get('models', [])
    
    if not models:
        logger.warning('No models provided in event')
        if is_custom_resource:
            return {
                'PhysicalResourceId': 'initial-quota-fetch',
                'Data': {}
            }
        else:
            return {'statusCode': 200, 'body': 'No models to process'}
    
    logger.info(f'Processing quotas for {len(models)} models')
    
    try:
        # Extract unique quota codes from all models
        quota_codes = set()
        for model in models:
            if isinstance(model, dict):
                token_quota_code = model.get('tokenQuotaCode')
                request_quota_code = model.get('requestQuotaCode')
                
                if token_quota_code:
                    quota_codes.add(token_quota_code)
                if request_quota_code:
                    quota_codes.add(request_quota_code)
        
        quota_codes = list(quota_codes)
        logger.info(f'Need to fetch {len(quota_codes)} unique quota codes')
        
        # Fetch only the specific quotas we need (much faster than fetching all 960)
        all_quotas = get_specific_quotas('bedrock', quota_codes)
        
        # Track processing statistics
        processed_models = 0
        successful_metrics = 0
        metrics_to_publish = []
        
        # Process all models and prepare metrics for batch publishing
        for model in models:
            # Validate model structure
            if not isinstance(model, dict):
                logger.error(f'Invalid model data structure: {model}')
                continue
                
            model_id = model.get('modelId')
            token_quota_code = model.get('tokenQuotaCode')
            request_quota_code = model.get('requestQuotaCode')
            
            if not model_id:
                logger.error(f'Missing modelId in model data: {model}')
                continue
                
            processed_models += 1
            
            # Process token quota
            if token_quota_code:
                token_quota = all_quotas.get(token_quota_code)
                if token_quota:
                    metrics_to_publish.append({
                        'MetricName': 'TokenQuota',
                        'Dimensions': [
                            {
                                'Name': 'ModelId',
                                'Value': model_id
                            }
                        ],
                        'Value': token_quota,
                        'Unit': 'None',
                        'Timestamp': datetime.utcnow()
                    })
                    successful_metrics += 1
                else:
                    logger.warning(f"Token quota code {token_quota_code} not found for {model_id}")
            else:
                logger.warning(f"No token quota code provided for {model_id}")
            
            # Process request quota
            if request_quota_code:
                request_quota = all_quotas.get(request_quota_code)
                if request_quota:
                    metrics_to_publish.append({
                        'MetricName': 'RequestQuota',
                        'Dimensions': [
                            {
                                'Name': 'ModelId',
                                'Value': model_id
                            }
                        ],
                        'Value': request_quota,
                        'Unit': 'None',
                        'Timestamp': datetime.utcnow()
                    })
                    successful_metrics += 1
                else:
                    logger.warning(f"Request quota code {request_quota_code} not found for {model_id}")
            else:
                logger.warning(f"No request quota code provided for {model_id}")
        
        # Publish all metrics in batches
        if metrics_to_publish:
            logger.info(f'Publishing {len(metrics_to_publish)} metrics in batches')
            publish_metrics_batch(metrics_to_publish)
        
        # Log processing summary
        logger.info(f'Processing complete: {processed_models} models processed, '
                    f'{successful_metrics} metrics published successfully')
        
        # Return appropriate response based on event type
        if is_custom_resource:
            return {
                'PhysicalResourceId': 'initial-quota-fetch',
                'Data': {
                    'ProcessedModels': processed_models,
                    'SuccessfulMetrics': successful_metrics,
                    'UniqueQuotaCodesFetched': len(quota_codes),
                    'QuotasFound': len(all_quotas)
                }
            }
        else:
            return {
                'statusCode': 200, 
                'body': json.dumps({
                    'message': f'Quotas refreshed for {processed_models} models',
                    'processedModels': processed_models,
                    'successfulMetrics': successful_metrics,
                    'uniqueQuotaCodesFetched': len(quota_codes),
                    'quotasFound': len(all_quotas)
                })
            }
            
    except Exception as e:
        logger.error(f'Error processing quotas: {str(e)}')
        
        if is_custom_resource:
            # For Custom Resources, we need to return success even on error to avoid stack rollback
            # The error will be logged but won't fail the deployment
            return {
                'PhysicalResourceId': 'initial-quota-fetch',
                'Data': {
                    'Error': str(e),
                    'ProcessedModels': 0,
                    'SuccessfulMetrics': 0
                }
            }
        else:
            return {
                'statusCode': 500,
                'body': json.dumps({
                    'error': str(e),
                    'message': 'Failed to process quotas'
                })
            }